//there are two database newsDB and tweetDB. newsDB contains two relations, news(newsID, newsText) and famoususer(userName, year). tweetDB contains a graph twitterNetwork. 
use tweetDB as tdb, use newsDB as ndb,
create analysis topicDistributionAnalysis as (
newsEntity := NETokenizer(news.newsText, 'docID' = news.newsID, ['text' = news.newsText]);
// count_Matrix return tdm which is a martix, docMatID and termMatID which are relations with schema (docID, mid), and (term, mid), which returns document ids or terms with their correponding matrix index. 
Matrix tdm; Relation docMatID; Relation termMatID;
countMatrix(tdm, docMatID, termMatID, news.newsText, features = (get_vocubulary(news.newsText), []));

// topic model for all news. M_d is topic document matrix and M_t is term topic matrix. M_d is of dimension 100 \times N where N is the vocubulary size.
Matrix docTopicMat; Matrix topicTermMat;
topicModel(docTopicMat, topicTermMat, tdm, k = 10);

// get count matrix for famous people
mid[] := select docMatID.mid from docMatID, newsEntity where newsEntity.entityTerm in executeSQL(select userName from ndb.famoususer where year = 2019) and docMatID.did; = newsEntity.docID;
tdmFamous[] := [];
for (id in mid){
	tdmFamous.add(tdm[id, :]);
}
// topic model for news about famous people
Matrix docTopicMatFamous; Matrix topicTermMatFamous;
topicModel(docTopicMatFamous, topicTermMatFamous, tdmFamous, k = 10);
// community detection 
Relation userCommunity := twitterNetwork.executeCypher(louvain('User', 'Reply') RETURN userName AS user, community ORDER BY community) store; 
communities[] := select distinct community from userCommunity;
// for each community, get the tweets text when group members reply each other and do the topic model for each community. 
similarity := [];
for (c  in communities) {
	users[] := select user from userCommunity where community = c;
	Relation tweets := twitterNetwork.executeCypher(match (u1:User) - [r:Reply] - (u2:User) where u1 in $users and u2 in $users return r.tweet.text);
	Matrix tdmCom; Relation docMatIDCom; Relation termMatIDCom;
    countMatrix(tdmCom, docMatIDCom, termMatIDCom, tweets.text, features = (get_vocubulary(tweets), []));
	Matrix docTopicMatCom; topicTermMatCom;
	topicModel(docTopicMatCom, topicTermMatCom, tdmCom, k = 10);
	newMatrix := mergeFeatureMatrix(termMatID, termMatIDCom, feature = 'col');
	topicTermMatNew := featureMatrixReconstruct(topicTermMat, termMatID, newMatrix, feature = 'col');
	topicTermMatComNew := featureMatrixReconstruct(topicTermMatCom, termMatIDCom, newMatrix, feature = 'col');
	sim[] := matrixSimilarity(topicTermMatNew, topicTermMatComNew, function = 'optimal match');
	similarity.add(ave(sim));
}
communityID ï¼š= similarity.indexOf(max(similarity));
sim := matrixSimilarity(topicTermMat, topicTermMatFamous, function = 'optimal match');
);