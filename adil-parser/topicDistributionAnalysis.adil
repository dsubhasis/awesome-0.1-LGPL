use tweetDB as tdb;
use newsDB as news;
create analysis topicDistributionAnalysis as (

newsEntity := NETokenizer(news, docID=news.newsID, textInput=news.newsText);
feat := getVocubulary(news.newsText);
tdm, docMatID, termMatID := countMatrix(news, textInput=news.newsText, docID=news.newsID, feature=feat);
docTopicMat, topicTermMat := topicModel(tdm, k=10);
tempQ := executeSQL("select userName from ndb.famoususer where year = 2019");
mid[] := select docMatID.mid from docMatID, newsEntity where newsEntity.entityTerm in tempQ and docMatID.did = newsEntity.docID;
tdmFamous[] := [];
for id : mid{
    tempVct[] := GetMatrixElement(tdm, row=id);
    tdmFamous := tdmFamous.add(tempVct);
};
docTopicMatFamous, topicTermMatFamous := topicModel(tdmFamous, k=10);
Relation userCommunity := executeCypher("louvain('User', 'Reply') RETURN userName AS user, community ORDER BY community") store;
communities[] := select distinct(community) from userCommunity;
similarity := [];
for c  : communities {
    users[] := select user from userCommunity where community = c;
    Relation tweets := executeCypher("match (u1:User) - [r:Reply] - (u2:User) where u1 in $users and u2 in $users return r.tweet.text");
    features := getVocubulary(tweets);
    tdmCom, docMatIDCom, termMatIDCom := countMatrix(tweets, textInput=tweets.text, feature=features);
    docTopicMatCom, topicTermMatCom := topicModel(tdmCom, k=10);
    newMatrix := mergeFeatureMatrix(termMatID, termMatIDCom);
    topicTermMatNew := featureMatrixReconstruct(topicTermMat, termMatID, newMatrix, feature=col);
    topicTermMatComNew := featureMatrixReconstruct(topicTermMatCom, termMatIDCom, newMatrix, feature=col);
    sim[] := matrixSimilarity(topicTermMatNew, topicTermMatComNew, function=OptimalMatch);
    similarity := similarity.add(ave(sim));
};
communityID := similarity.indexOf(max(similarity));
);