//there are two database newsDB and tweetDB. newsDB contains two relations, news(newsID, newsText) and famoususer(userName, year). tweetDB contains a graph twitterNetwork. 
use tweetDB as tdb, use newsDB as ndb,
create analysis topicDistributionAnalysis as (
newsEntity := NETokenizer(news.newsText, 'docID' = news.newsID, ['text' = news.newsText]);
// count_Matrix return tdm which is a martix, docMatID and termMatID which are relations with schema (docID, mid), and (term, mid), which returns document ids or terms with their correponding matrix index. 

tdm, docMatID, termMatID := count_Matrix(news.newsText, features = (get_vocubulary(news.newsText), []));

// topic model for all news. M_d is topic document matrix and M_t is term topic matrix. M_d is of dimension 100 \times N where N is the vocubulary size.

M_d, M_t := topic_model(tdm, k = 10);

// get count matrix for famous people
mid[] := select docMatID.mid from docMatID, newsEntity where newsEntity.entityTerm in executeSQL(select userName from ndb.famoususer where year = 2019) and docMatID.did = newsEntity.docID;
tdm_famous[] = [] 
for (id in mid){
	tdm_famous.add(tdm[id, :])
}
// topic model for news about famous people
M_d_famous, M_t_famous := topic_model(tdm_famous, k = 10);
// community detection 
Relation userCommunity := twitterNetwork.executeCypher(louvain('User', 'Reply') RETURN userName AS user, community ORDER BY community) store; 
communities[] := select distinct community from userCommunity;
// for each community, get the tweets text when group members reply each other and do the topic model for each community. 
similarity := []
//termMatID_c[], M_t_c := [], [];
for (c  in communities) {
	users[] := select user from userCommunity where community = c;
	Collection tweets := twitterNetwork.executeCypher(match ([u1:User] - [r:Reply] - [u2:User]) where u1 in $users and u2 in $users return r.tweet);
	Relation tweets := Scan(tweets);
	tdm_c, dMID_c, tMID := count_Matrix(tweets.text, features = (get_vocubulary(tweets), []));
	M_dc, M_tc := topic_model(tdm_c, k = 10);
	newMatrix := mergeFeatureMatrix(termMatID, tMID);
	M_t_new := featureMatrixReconstruct(M_t, termMatID, newMatrix);
	M_tc_new := featureMatrixReconstruct(M_tc, T, newMatrix);
	sim := matrixSimilarity(M_t_new, M_tc_new, function = 'optimal match');
	similarity.add(sim);
}
communityID ï¼š= similarity.indexOf(max(similarity));
sim := matrixSimilarity(M_t, M_t_famous, function = 'optimal match');
);